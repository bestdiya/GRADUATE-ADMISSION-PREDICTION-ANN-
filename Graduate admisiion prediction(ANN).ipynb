{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc6361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5612c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf7187b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "395         396        324          110                  3  3.5   3.5  9.04   \n",
       "396         397        325          107                  3  3.0   3.5  9.11   \n",
       "397         398        330          116                  4  5.0   4.5  9.45   \n",
       "398         399        312          103                  3  3.5   4.0  8.78   \n",
       "399         400        333          117                  4  5.0   4.0  9.66   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "395         1              0.82  \n",
       "396         1              0.84  \n",
       "397         1              0.91  \n",
       "398         0              0.67  \n",
       "399         1              0.95  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af8f0afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    133\n",
       "2    107\n",
       "4     74\n",
       "5     60\n",
       "1     26\n",
       "Name: University Rating, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['University Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a75d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.3 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c8ac9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab7e8787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "495         496        332          108                  5  4.5   4.0  9.02   \n",
       "496         497        337          117                  5  5.0   5.0  9.87   \n",
       "497         498        330          120                  5  4.5   5.0  9.56   \n",
       "498         499        312          103                  4  4.0   5.0  8.43   \n",
       "499         500        327          113                  4  4.5   4.5  9.04   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "495         1              0.87  \n",
       "496         1              0.96  \n",
       "497         1              0.93  \n",
       "498         0              0.73  \n",
       "499         0              0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b710cc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Serial No.   GRE Score  TOEFL Score  University Rating         SOP  \\\n",
       "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
       "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
       "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
       "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
       "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
       "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
       "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
       "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
       "\n",
       "            LOR         CGPA    Research  Chance of Admit   \n",
       "count  500.00000  500.000000  500.000000         500.00000  \n",
       "mean     3.48400    8.576440    0.560000           0.72174  \n",
       "std      0.92545    0.604813    0.496884           0.14114  \n",
       "min      1.00000    6.800000    0.000000           0.34000  \n",
       "25%      3.00000    8.127500    0.000000           0.63000  \n",
       "50%      3.50000    8.560000    1.000000           0.72000  \n",
       "75%      4.00000    9.040000    1.000000           0.82000  \n",
       "max      5.00000    9.920000    1.000000           0.97000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f2cff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.           0\n",
       "GRE Score            0\n",
       "TOEFL Score          0\n",
       "University Rating    0\n",
       "SOP                  0\n",
       "LOR                  0\n",
       "CGPA                 0\n",
       "Research             0\n",
       "Chance of Admit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d54a72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=['Serial No.'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d9cd75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "495        332          108                  5  4.5   4.0  9.02         1   \n",
       "496        337          117                  5  5.0   5.0  9.87         1   \n",
       "497        330          120                  5  4.5   5.0  9.56         1   \n",
       "498        312          103                  4  4.0   5.0  8.43         0   \n",
       "499        327          113                  4  4.5   4.5  9.04         0   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "495              0.87  \n",
       "496              0.96  \n",
       "497              0.93  \n",
       "498              0.73  \n",
       "499              0.84  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "262baa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e70eb342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "395        324          110                  3  3.5   3.5  9.04         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "397        330          116                  4  5.0   4.5  9.45         1\n",
       "398        312          103                  3  3.5   4.0  8.78         0\n",
       "399        333          117                  4  5.0   4.0  9.66         1\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1fb3cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "395    0.82\n",
       "396    0.84\n",
       "397    0.91\n",
       "398    0.67\n",
       "399    0.95\n",
       "Name: Chance of Admit , Length: 400, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c467348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f63e11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>301</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>334</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>305</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>307</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>318</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "93         301           97                  2  3.0   3.0  7.88         1\n",
       "23         334          119                  5  5.0   4.5  9.70         1\n",
       "299        305          112                  3  3.0   3.5  8.65         0\n",
       "13         307          109                  3  4.0   3.0  8.00         1\n",
       "90         318          106                  2  4.0   4.0  7.92         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[320 rows x 7 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07f41742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>324</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>299</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>316</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>300</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>309</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>305</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "398        312          103                  3  3.5   4.0  8.78         0\n",
       "125        300          100                  3  2.0   3.0  8.66         1\n",
       "328        324          112                  4  4.0   3.5  8.77         1\n",
       "339        324          107                  5  3.5   4.0  8.66         1\n",
       "172        322          110                  4  4.0   5.0  9.13         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "347        299           94                  1  1.0   1.0  7.34         0\n",
       "41         316          105                  2  2.5   2.5  8.20         1\n",
       "180        300          104                  3  3.5   3.0  8.16         0\n",
       "132        309          105                  5  3.5   3.5  8.56         0\n",
       "224        305          105                  2  3.0   2.0  8.23         0\n",
       "\n",
       "[80 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a74c7dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93     0.44\n",
       "23     0.95\n",
       "299    0.71\n",
       "13     0.62\n",
       "90     0.64\n",
       "       ... \n",
       "255    0.79\n",
       "72     0.93\n",
       "396    0.84\n",
       "235    0.88\n",
       "37     0.58\n",
       "Name: Chance of Admit , Length: 320, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91d37166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398    0.67\n",
       "125    0.64\n",
       "328    0.80\n",
       "339    0.81\n",
       "172    0.86\n",
       "       ... \n",
       "347    0.42\n",
       "41     0.49\n",
       "180    0.71\n",
       "132    0.71\n",
       "224    0.67\n",
       "Name: Chance of Admit , Length: 80, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd822dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "589e38b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22      , 0.17857143, 0.25      , ..., 0.42857143, 0.25      ,\n",
       "        1.        ],\n",
       "       [0.88      , 0.96428571, 1.        , ..., 0.85714286, 0.91911765,\n",
       "        1.        ],\n",
       "       [0.3       , 0.71428571, 0.5       , ..., 0.57142857, 0.53308824,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.70220588,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.74632353,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.22058824,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b39efac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44      , 0.34615385, 0.5       , 0.625     , 0.75      ,\n",
       "        0.65562914, 0.        ],\n",
       "       [0.2       , 0.23076923, 0.5       , 0.25      , 0.5       ,\n",
       "        0.61589404, 1.        ],\n",
       "       [0.68      , 0.69230769, 0.75      , 0.75      , 0.625     ,\n",
       "        0.65231788, 1.        ],\n",
       "       [0.68      , 0.5       , 1.        , 0.625     , 0.75      ,\n",
       "        0.61589404, 1.        ],\n",
       "       [0.64      , 0.61538462, 0.75      , 0.75      , 1.        ,\n",
       "        0.77152318, 1.        ],\n",
       "       [0.36      , 0.46153846, 0.5       , 0.5       , 0.5       ,\n",
       "        0.47682119, 0.        ],\n",
       "       [0.4       , 0.46153846, 0.25      , 0.625     , 0.375     ,\n",
       "        0.50662252, 0.        ],\n",
       "       [0.2       , 0.30769231, 0.25      , 0.125     , 0.25      ,\n",
       "        0.35430464, 0.        ],\n",
       "       [0.4       , 0.19230769, 0.25      , 0.125     , 0.25      ,\n",
       "        0.16556291, 0.        ],\n",
       "       [1.        , 0.69230769, 0.75      , 1.        , 0.875     ,\n",
       "        0.94701987, 1.        ],\n",
       "       [0.62      , 0.65384615, 0.75      , 0.75      , 0.75      ,\n",
       "        0.71854305, 1.        ],\n",
       "       [0.92      , 0.96153846, 0.75      , 0.875     , 0.75      ,\n",
       "        0.93377483, 1.        ],\n",
       "       [0.82      , 0.80769231, 1.        , 0.875     , 0.625     ,\n",
       "        0.84768212, 1.        ],\n",
       "       [0.5       , 0.38461538, 0.5       , 0.5       , 0.375     ,\n",
       "        0.50662252, 0.        ],\n",
       "       [0.66      , 0.38461538, 0.5       , 0.75      , 0.75      ,\n",
       "        0.54304636, 1.        ],\n",
       "       [0.44      , 0.61538462, 0.25      , 0.625     , 0.5       ,\n",
       "        0.57284768, 0.        ],\n",
       "       [0.68      , 0.61538462, 0.5       , 0.625     , 0.5       ,\n",
       "        0.8013245 , 1.        ],\n",
       "       [0.8       , 0.80769231, 1.        , 0.875     , 0.5       ,\n",
       "        0.8410596 , 1.        ],\n",
       "       [0.12      , 0.03846154, 0.25      , 0.5       , 0.25      ,\n",
       "        0.24503311, 1.        ],\n",
       "       [0.36      , 0.57692308, 0.25      , 0.5       , 0.75      ,\n",
       "        0.54635762, 0.        ],\n",
       "       [0.4       , 0.46153846, 0.75      , 0.125     , 0.375     ,\n",
       "        0.51655629, 0.        ],\n",
       "       [0.68      , 0.80769231, 0.5       , 0.625     , 0.5       ,\n",
       "        0.64900662, 1.        ],\n",
       "       [0.68      , 0.61538462, 0.75      , 0.875     , 0.75      ,\n",
       "        0.7781457 , 1.        ],\n",
       "       [0.82      , 0.88461538, 0.75      , 0.875     , 1.        ,\n",
       "        0.86754967, 1.        ],\n",
       "       [0.58      , 0.46153846, 0.5       , 0.75      , 0.5       ,\n",
       "        0.39735099, 1.        ],\n",
       "       [0.16      , 0.15384615, 0.25      , 0.75      , 0.5       ,\n",
       "        0.40728477, 0.        ],\n",
       "       [0.6       , 0.38461538, 0.5       , 0.625     , 0.875     ,\n",
       "        0.50993377, 1.        ],\n",
       "       [0.5       , 0.61538462, 0.25      , 0.625     , 0.5       ,\n",
       "        0.54966887, 1.        ],\n",
       "       [0.88      , 0.84615385, 0.75      , 0.75      , 0.625     ,\n",
       "        0.90728477, 1.        ],\n",
       "       [0.4       , 0.30769231, 0.5       , 0.625     , 0.75      ,\n",
       "        0.40397351, 1.        ],\n",
       "       [1.        , 1.        , 0.75      , 1.        , 1.        ,\n",
       "        0.89403974, 1.        ],\n",
       "       [0.48      , 0.34615385, 0.25      , 0.25      , 0.5       ,\n",
       "        0.46688742, 0.        ],\n",
       "       [0.64      , 0.61538462, 1.        , 0.875     , 0.75      ,\n",
       "        0.71854305, 0.        ],\n",
       "       [0.64      , 0.61538462, 0.5       , 0.625     , 0.5       ,\n",
       "        0.71523179, 1.        ],\n",
       "       [0.62      , 0.57692308, 0.5       , 0.5       , 0.75      ,\n",
       "        0.46357616, 1.        ],\n",
       "       [0.44      , 0.42307692, 0.5       , 0.25      , 0.5       ,\n",
       "        0.40397351, 1.        ],\n",
       "       [0.2       , 0.19230769, 0.        , 0.5       , 0.25      ,\n",
       "        0.        , 1.        ],\n",
       "       [0.48      , 0.46153846, 0.25      , 0.75      , 0.625     ,\n",
       "        0.48013245, 0.        ],\n",
       "       [0.8       , 0.84615385, 0.75      , 1.        , 0.875     ,\n",
       "        0.87748344, 1.        ],\n",
       "       [0.56      , 0.57692308, 0.        , 0.625     , 0.625     ,\n",
       "        0.76821192, 0.        ],\n",
       "       [0.74      , 0.73076923, 0.75      , 0.875     , 1.        ,\n",
       "        0.77483444, 0.        ],\n",
       "       [0.16      , 0.42307692, 0.5       , 0.625     , 0.75      ,\n",
       "        0.57615894, 0.        ],\n",
       "       [0.88      , 0.88461538, 1.        , 0.75      , 0.875     ,\n",
       "        0.75165563, 1.        ],\n",
       "       [0.28      , 0.42307692, 0.25      , 0.5       , 0.5       ,\n",
       "        0.46357616, 1.        ],\n",
       "       [0.9       , 0.88461538, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        ],\n",
       "       [0.58      , 0.34615385, 0.75      , 0.875     , 0.625     ,\n",
       "        0.61589404, 0.        ],\n",
       "       [0.6       , 0.53846154, 0.5       , 0.625     , 0.75      ,\n",
       "        0.54304636, 1.        ],\n",
       "       [0.7       , 0.69230769, 0.75      , 0.75      , 0.75      ,\n",
       "        0.72847682, 1.        ],\n",
       "       [0.3       , 0.42307692, 0.25      , 0.5       , 0.75      ,\n",
       "        0.44039735, 0.        ],\n",
       "       [0.36      , 0.34615385, 0.25      , 0.5       , 0.625     ,\n",
       "        0.55960265, 0.        ],\n",
       "       [0.7       , 0.69230769, 0.75      , 0.625     , 0.625     ,\n",
       "        0.70198675, 0.        ],\n",
       "       [0.12      , 0.19230769, 0.25      , 0.375     , 0.375     ,\n",
       "        0.40728477, 0.        ],\n",
       "       [0.68      , 0.65384615, 0.75      , 0.5       , 0.5       ,\n",
       "        0.73178808, 1.        ],\n",
       "       [0.56      , 0.61538462, 0.5       , 0.75      , 0.5       ,\n",
       "        0.66225166, 0.        ],\n",
       "       [0.4       , 0.5       , 0.5       , 0.625     , 0.625     ,\n",
       "        0.6192053 , 0.        ],\n",
       "       [0.92      , 0.92307692, 1.        , 0.875     , 0.75      ,\n",
       "        0.79139073, 1.        ],\n",
       "       [0.08      , 0.03846154, 0.        , 0.125     , 0.125     ,\n",
       "        0.2781457 , 0.        ],\n",
       "       [0.        , 0.38461538, 0.75      , 0.25      , 0.375     ,\n",
       "        0.21854305, 0.        ],\n",
       "       [0.72      , 0.30769231, 0.75      , 1.        , 1.        ,\n",
       "        0.64900662, 1.        ],\n",
       "       [0.64      , 0.61538462, 0.5       , 0.75      , 1.        ,\n",
       "        0.60927152, 1.        ],\n",
       "       [0.96      , 0.88461538, 0.75      , 0.625     , 0.875     ,\n",
       "        0.8807947 , 1.        ],\n",
       "       [0.52      , 0.5       , 0.25      , 0.625     , 0.625     ,\n",
       "        0.60927152, 1.        ],\n",
       "       [0.8       , 0.73076923, 1.        , 1.        , 0.75      ,\n",
       "        0.83112583, 1.        ],\n",
       "       [0.52      , 0.42307692, 0.5       , 0.5       , 0.625     ,\n",
       "        0.63907285, 0.        ],\n",
       "       [0.14      , 0.15384615, 0.25      , 0.375     , 0.5       ,\n",
       "        0.28807947, 0.        ],\n",
       "       [0.68      , 0.73076923, 0.75      , 0.875     , 0.75      ,\n",
       "        0.6589404 , 0.        ],\n",
       "       [0.78      , 0.65384615, 0.75      , 0.875     , 0.875     ,\n",
       "        0.78807947, 1.        ],\n",
       "       [0.6       , 0.38461538, 0.5       , 0.5       , 0.625     ,\n",
       "        0.64238411, 1.        ],\n",
       "       [0.32      , 0.42307692, 0.25      , 0.375     , 0.5       ,\n",
       "        0.47019868, 1.        ],\n",
       "       [0.42      , 0.5       , 0.75      , 0.875     , 0.875     ,\n",
       "        0.72847682, 1.        ],\n",
       "       [0.68      , 0.61538462, 0.75      , 0.5       , 0.625     ,\n",
       "        0.71854305, 1.        ],\n",
       "       [0.28      , 0.23076923, 0.75      , 0.125     , 0.375     ,\n",
       "        0.34437086, 0.        ],\n",
       "       [0.56      , 0.5       , 0.5       , 0.5       , 0.625     ,\n",
       "        0.48675497, 1.        ],\n",
       "       [0.34      , 0.42307692, 0.25      , 0.25      , 0.625     ,\n",
       "        0.43046358, 0.        ],\n",
       "       [0.74      , 0.65384615, 0.75      , 0.75      , 0.875     ,\n",
       "        0.72847682, 1.        ],\n",
       "       [0.18      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.17880795, 0.        ],\n",
       "       [0.52      , 0.42307692, 0.25      , 0.375     , 0.375     ,\n",
       "        0.46357616, 1.        ],\n",
       "       [0.2       , 0.38461538, 0.5       , 0.625     , 0.5       ,\n",
       "        0.45033113, 0.        ],\n",
       "       [0.38      , 0.42307692, 1.        , 0.625     , 0.625     ,\n",
       "        0.58278146, 0.        ],\n",
       "       [0.3       , 0.42307692, 0.25      , 0.5       , 0.25      ,\n",
       "        0.47350993, 0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "427652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84da7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(7,activation='relu',input_dim=7))\n",
    "model.add(Dense(7,activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f95c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120 (480.00 Byte)\n",
      "Trainable params: 120 (480.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "afd2d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c432dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 38ms/step - loss: 0.2213 - val_loss: 0.1924\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1584 - val_loss: 0.1293\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1081 - val_loss: 0.0809\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0715 - val_loss: 0.0496\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0487 - val_loss: 0.0332\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0362 - val_loss: 0.0255\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0292 - val_loss: 0.0213\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0249 - val_loss: 0.0190\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0217 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0191 - val_loss: 0.0164\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0173 - val_loss: 0.0158\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0160 - val_loss: 0.0154\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0145\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0139\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0134\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0102\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0090 - val_loss: 0.0096\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0093\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0088\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0086\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0081 - val_loss: 0.0084\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0082\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0080\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 0.0049\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0049\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train_scaled,y_train,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c42ae505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0d822d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7028638505981252"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c633fd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fcabd56210>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf0c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
